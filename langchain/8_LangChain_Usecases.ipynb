{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Dhaval Antala\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.0\n",
      "IPython version      : 8.25.0\n",
      "\n",
      "langchain: 0.2.5\n",
      "openai   : 1.32.0\n",
      "\n",
      "Compiler    : Clang 12.0.0 \n",
      "OS          : Darwin\n",
      "Release     : 23.5.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Dhaval Antala\" -vmp langchain,openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ❓ Question Answering Over Docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThere are many people from Gujarat, a state in western India. Some famous people from Gujarat include Mahatma Gandhi, Narendra Modi, and Amitabh Bachchan.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"who is from Gujarat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Dhaval is from Gujarat.\n",
    "Sudarshan is from UP.\n",
    "Mikko is from AP.\n",
    "Khoa is from Delhi.\n",
    "\"\"\"\n",
    "\n",
    "question = \"who is from Gujarat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dhaval is from Gujarat.\n"
     ]
    }
   ],
   "source": [
    "output = llm(context + question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 💬 Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an assistant trained by OpenAI.\n",
    "Your goal is to provide help just with foods.\n",
    "Don't provide answer other than food related topics. \n",
    "Just output \"I don't know\" if other topics are asked.\n",
    "\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variable = [\"history\", \"human_input\"],\n",
    "    template = template)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chatgpt_chain = LLMChain(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    prompt=prompt,\n",
    "    verbose = True,\n",
    "    memory = ConversationBufferWindowMemory(memory_key=\"history\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an assistant trained by OpenAI.\n",
      "Your goal is to provide help just with foods.\n",
      "Don't provide answer other than food related topics. \n",
      "Just output \"I don't know\" if other topics are asked.\n",
      "\n",
      "\n",
      "Human: What is Python?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " I don't know.\n"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(\n",
    "    human_input=\"What is Python?\"\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an assistant trained by OpenAI.\n",
      "Your goal is to provide help just with foods.\n",
      "Don't provide answer other than food related topics. \n",
      "Just output \"I don't know\" if other topics are asked.\n",
      "\n",
      "Human: What is Python?\n",
      "AI:  I don't know.\n",
      "Human: Which fruit is better, apple or orange ?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " I don't know.\n"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(human_input=\"Which fruit is better, apple or orange ?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an assistant trained by OpenAI.\n",
      "Your goal is to provide help just with foods.\n",
      "Don't provide answer other than food related topics. \n",
      "Just output \"I don't know\" if other topics are asked.\n",
      "\n",
      "Human: What is Python?\n",
      "AI:  I don't know.\n",
      "Human: Which fruit is better, apple or orange ?\n",
      "AI:  I don't know.\n",
      "Human: What about apple and samsung ?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " I don't know.\n"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(human_input=\"What about apple and samsung ?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an assistant trained by OpenAI.\n",
      "Your goal is to provide help just with foods.\n",
      "Don't provide answer other than food related topics. \n",
      "Just output \"I don't know\" if other topics are asked.\n",
      "\n",
      "Human: What is Python?\n",
      "AI:  I don't know.\n",
      "Human: Which fruit is better, apple or orange ?\n",
      "AI:  I don't know.\n",
      "Human: What about apple and samsung ?\n",
      "AI:  I don't know.\n",
      "Human: What is the first question I asked you ?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " I don't know.\n"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(human_input=\"What is the first question I asked you ?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 📚 Querying Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain, SQLDatabaseSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_uri = 'mysql+mysqlconnector://root:pass1234@localhost:3306/chinook'\n",
    "db = SQLDatabase.from_uri(mysql_uri)\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm=llm, db=db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many employees are there?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(*) FROM Employee\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(8,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThere are 8 employees.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 8 employees.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"How many employees are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 🔌 Interacting with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- APIs sre powerfull because if you need to perform some action or talk to data from behind an API, we need LLM to interact with it.\n",
    "- Lets go through on example using [Open-Meteo](https://open-meteo.com/) which is a free weather api.\n",
    "- Open-Meteo is an open-source weather API with free access for non-comercial use. No API key required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.api.base import APIChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.api import open_meteo_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_new = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\n",
    "chain_new = APIChain.from_llm_and_api_docs(\n",
    "    llm,\n",
    "    open_meteo_docs.OPEN_METEO_DOCS,\n",
    "    verbose=True,\n",
    "    limit_to_domains=[\"https://api.open-meteo.com/\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://api.open-meteo.com/v1/forecast?latitude=49.489591&longitude=8.461330&hourly=temperature_2m&current_weather=true&temperature_unit=celsius&timezone=auto\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{\"latitude\":49.48,\"longitude\":8.459999,\"generationtime_ms\":0.10204315185546875,\"utc_offset_seconds\":7200,\"timezone\":\"Europe/Berlin\",\"timezone_abbreviation\":\"CEST\",\"elevation\":102.0,\"current_weather_units\":{\"time\":\"iso8601\",\"interval\":\"seconds\",\"temperature\":\"°C\",\"windspeed\":\"km/h\",\"winddirection\":\"°\",\"is_day\":\"\",\"weathercode\":\"wmo code\"},\"current_weather\":{\"time\":\"2024-06-26T11:15\",\"interval\":900,\"temperature\":25.6,\"windspeed\":3.1,\"winddirection\":234,\"is_day\":1,\"weathercode\":0},\"hourly_units\":{\"time\":\"iso8601\",\"temperature_2m\":\"°C\"},\"hourly\":{\"time\":[\"2024-06-26T00:00\",\"2024-06-26T01:00\",\"2024-06-26T02:00\",\"2024-06-26T03:00\",\"2024-06-26T04:00\",\"2024-06-26T05:00\",\"2024-06-26T06:00\",\"2024-06-26T07:00\",\"2024-06-26T08:00\",\"2024-06-26T09:00\",\"2024-06-26T10:00\",\"2024-06-26T11:00\",\"2024-06-26T12:00\",\"2024-06-26T13:00\",\"2024-06-26T14:00\",\"2024-06-26T15:00\",\"2024-06-26T16:00\",\"2024-06-26T17:00\",\"2024-06-26T18:00\",\"2024-06-26T19:00\",\"2024-06-26T20:00\",\"2024-06-26T21:00\",\"2024-06-26T22:00\",\"2024-06-26T23:00\",\"2024-06-27T00:00\",\"2024-06-27T01:00\",\"2024-06-27T02:00\",\"2024-06-27T03:00\",\"2024-06-27T04:00\",\"2024-06-27T05:00\",\"2024-06-27T06:00\",\"2024-06-27T07:00\",\"2024-06-27T08:00\",\"2024-06-27T09:00\",\"2024-06-27T10:00\",\"2024-06-27T11:00\",\"2024-06-27T12:00\",\"2024-06-27T13:00\",\"2024-06-27T14:00\",\"2024-06-27T15:00\",\"2024-06-27T16:00\",\"2024-06-27T17:00\",\"2024-06-27T18:00\",\"2024-06-27T19:00\",\"2024-06-27T20:00\",\"2024-06-27T21:00\",\"2024-06-27T22:00\",\"2024-06-27T23:00\",\"2024-06-28T00:00\",\"2024-06-28T01:00\",\"2024-06-28T02:00\",\"2024-06-28T03:00\",\"2024-06-28T04:00\",\"2024-06-28T05:00\",\"2024-06-28T06:00\",\"2024-06-28T07:00\",\"2024-06-28T08:00\",\"2024-06-28T09:00\",\"2024-06-28T10:00\",\"2024-06-28T11:00\",\"2024-06-28T12:00\",\"2024-06-28T13:00\",\"2024-06-28T14:00\",\"2024-06-28T15:00\",\"2024-06-28T16:00\",\"2024-06-28T17:00\",\"2024-06-28T18:00\",\"2024-06-28T19:00\",\"2024-06-28T20:00\",\"2024-06-28T21:00\",\"2024-06-28T22:00\",\"2024-06-28T23:00\",\"2024-06-29T00:00\",\"2024-06-29T01:00\",\"2024-06-29T02:00\",\"2024-06-29T03:00\",\"2024-06-29T04:00\",\"2024-06-29T05:00\",\"2024-06-29T06:00\",\"2024-06-29T07:00\",\"2024-06-29T08:00\",\"2024-06-29T09:00\",\"2024-06-29T10:00\",\"2024-06-29T11:00\",\"2024-06-29T12:00\",\"2024-06-29T13:00\",\"2024-06-29T14:00\",\"2024-06-29T15:00\",\"2024-06-29T16:00\",\"2024-06-29T17:00\",\"2024-06-29T18:00\",\"2024-06-29T19:00\",\"2024-06-29T20:00\",\"2024-06-29T21:00\",\"2024-06-29T22:00\",\"2024-06-29T23:00\",\"2024-06-30T00:00\",\"2024-06-30T01:00\",\"2024-06-30T02:00\",\"2024-06-30T03:00\",\"2024-06-30T04:00\",\"2024-06-30T05:00\",\"2024-06-30T06:00\",\"2024-06-30T07:00\",\"2024-06-30T08:00\",\"2024-06-30T09:00\",\"2024-06-30T10:00\",\"2024-06-30T11:00\",\"2024-06-30T12:00\",\"2024-06-30T13:00\",\"2024-06-30T14:00\",\"2024-06-30T15:00\",\"2024-06-30T16:00\",\"2024-06-30T17:00\",\"2024-06-30T18:00\",\"2024-06-30T19:00\",\"2024-06-30T20:00\",\"2024-06-30T21:00\",\"2024-06-30T22:00\",\"2024-06-30T23:00\",\"2024-07-01T00:00\",\"2024-07-01T01:00\",\"2024-07-01T02:00\",\"2024-07-01T03:00\",\"2024-07-01T04:00\",\"2024-07-01T05:00\",\"2024-07-01T06:00\",\"2024-07-01T07:00\",\"2024-07-01T08:00\",\"2024-07-01T09:00\",\"2024-07-01T10:00\",\"2024-07-01T11:00\",\"2024-07-01T12:00\",\"2024-07-01T13:00\",\"2024-07-01T14:00\",\"2024-07-01T15:00\",\"2024-07-01T16:00\",\"2024-07-01T17:00\",\"2024-07-01T18:00\",\"2024-07-01T19:00\",\"2024-07-01T20:00\",\"2024-07-01T21:00\",\"2024-07-01T22:00\",\"2024-07-01T23:00\",\"2024-07-02T00:00\",\"2024-07-02T01:00\",\"2024-07-02T02:00\",\"2024-07-02T03:00\",\"2024-07-02T04:00\",\"2024-07-02T05:00\",\"2024-07-02T06:00\",\"2024-07-02T07:00\",\"2024-07-02T08:00\",\"2024-07-02T09:00\",\"2024-07-02T10:00\",\"2024-07-02T11:00\",\"2024-07-02T12:00\",\"2024-07-02T13:00\",\"2024-07-02T14:00\",\"2024-07-02T15:00\",\"2024-07-02T16:00\",\"2024-07-02T17:00\",\"2024-07-02T18:00\",\"2024-07-02T19:00\",\"2024-07-02T20:00\",\"2024-07-02T21:00\",\"2024-07-02T22:00\",\"2024-07-02T23:00\"],\"temperature_2m\":[21.3,20.2,19.3,18.7,18.1,17.8,17.5,18.0,19.8,21.5,23.5,25.2,27.0,28.5,29.6,30.5,31.0,30.8,30.4,28.4,27.0,25.6,24.0,23.0,22.2,21.6,21.1,20.7,20.7,20.6,20.2,20.5,21.7,23.6,25.6,27.5,29.6,31.2,31.5,33.0,32.2,32.1,32.2,31.4,30.6,27.6,26.6,25.7,24.4,23.3,22.5,21.7,21.2,20.8,20.6,21.2,22.6,22.2,23.5,25.1,26.5,27.3,27.7,28.4,28.1,28.0,27.7,27.2,26.2,24.2,22.5,21.3,19.3,18.4,17.9,17.6,17.3,17.0,17.1,17.6,18.5,19.9,21.8,23.7,25.9,28.2,30.0,31.2,31.9,32.1,32.0,31.4,30.0,26.9,22.8,19.8,18.5,18.3,18.1,17.9,17.8,17.9,18.1,18.5,19.0,19.5,20.1,20.3,19.8,18.9,18.3,18.3,18.7,18.8,18.7,18.5,18.2,17.7,17.1,16.7,16.6,16.6,16.6,16.2,16.0,15.8,15.7,15.7,15.9,16.6,17.6,18.4,18.7,18.8,18.6,17.9,16.9,16.1,15.6,15.2,14.9,14.4,13.9,13.5,13.3,13.2,13.1,13.0,12.9,12.9,13.0,13.2,13.4,13.6,14.0,14.4,15.1,15.9,16.8,17.9,19.0,19.6,19.4,18.7,17.9,17.2,16.4,15.7]}}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The current temperature in Mahhiem, Germany is 25.6 degrees Celsius. This information was obtained from the API url: https://api.open-meteo.com/v1/forecast?latitude=49.489591&longitude=8.461330&hourly=temperature_2m&current_weather=true&temperature_unit=celsius&timezone=auto.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.run('What is the weather like right now in Mahhiem, Germany in degrees Celsius?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 📝 Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating smaller summary from longer documents.\n",
    "- There are different chain types\n",
    "- Many ways how you can interact with PDF.\n",
    "- Summarization can be done from couple of sentences to entire book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph summarization\n",
    "from langchain import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Please provide a summary of the following text.\n",
    "Provide answer in simple terms and max lenght of 30 words.\n",
    "\n",
    "TEXT:\n",
    "A common use case is wanting to summarize long documents. This naturally runs into \\\n",
    "the context window limitations. Unlike in question-answering, you can't just do some \\\n",
    "semantic search hacks to only select the chunks of text most relevant to the question \\\n",
    "(because, in this case, there is no particular question - you want to summarize everything). So what do you do then?\n",
    "\n",
    "The most common way around this is to split the documents into chunks and then do \\\n",
    "summarization in a recursive manner. By this we mean you first summarize each chunk \\\n",
    "by itself, then you group the summaries into chunks and summarize each chunk of summaries, and continue doing that until only one is left.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prompt has 159 tokens\n"
     ]
    }
   ],
   "source": [
    "num_tokens = llm.get_num_tokens(prompt)\n",
    "print (f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarizing long documents can be challenging due to context window limitations. To overcome this, the document is split into chunks and recursively summarized until only one remains.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('../langchain/data/human-nutrition-text.pdf')\n",
    "doc=loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis chapter introduces the Food Science and Human Nutrition Program at the University of Hawaii at Manoa and emphasizes the importance of a strong foundation in both the program and in life. It covers basic concepts in nutrition, the six classes of nutrients, and the role of macronutrients in providing energy and regulating bodily functions. Water is also considered a macronutrient, while micronutrients such as minerals and vitamins play important roles in the body.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.run(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 📤 Extraction\n",
    "- Extracting something.\n",
    "- Extraction is related to [output parsing](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/) which are responsible for instructing LLMs to respond in a specific format. \n",
    "- For deep divem LangChain recommends checking [KOR](https://eyurtsev.github.io/kor/index.html) library which uses LangChain chain and OutputParser abstractions but allows deep dive on allowing extraction of more complicated schemas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Given a random sentence which contains animals name, extract animal names and assign an emoji to that and return just the animal name with emoji.\n",
    "\"\"\"\n",
    "\n",
    "animal_names = \"\"\"\n",
    "Dog, cat and rabbit are in the garden.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐶 Dog\n",
      "🐱 Cat\n",
      "🐰 Rabbit\n"
     ]
    }
   ],
   "source": [
    "prompt = (instructions + animal_names)\n",
    "output = chat_model([HumanMessage(content=prompt)])\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through one example from kor too.\n",
    "\n",
    "- Kor is a thin wrapper on top of LLMs that helps to extract structured data using LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor.extraction import create_extraction_chain\n",
    "from kor.nodes import Object, Text, Number\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Object(\n",
    "    id=\"person\",\n",
    "    description=\"Personal information\",\n",
    "    examples=[\n",
    "        (\"Alice and Bob are friends\", [{\"first_name\": \"Alice\"}, {\"first_name\": \"Bob\"}])\n",
    "    ],\n",
    "    attributes=[\n",
    "        Text(\n",
    "            id=\"first_name\",\n",
    "            description=\"The first name of a person.\",\n",
    "        )\n",
    "    ],\n",
    "    many=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a langchain llm and create a chain\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "chain = create_extraction_chain(llm, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person': [{'first_name': 'Bobby'}, {'first_name': 'Joe'}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract\n",
    "chain.run((\"My name is Bobby. My brother's name Joe.\"))[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'person': [{'first_name': 'Bobby'}, {'first_name': 'Joe'}]},\n",
       " 'raw': 'first_name\\nBobby\\nJoe',\n",
       " 'errors': [],\n",
       " 'validated_data': {}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run((\"My name is Bobby. My brother's name Joe.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person': [{'first_name': 'Bobby'},\n",
       "  {'first_name': 'Joe'},\n",
       "  {'first_name': 'Stephen'}]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run((\"My name is Bobby. My brother's name Joe. My another brother's name is Stephen\"))['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 🧐 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation after creating chins/agents internally as well as application building on top of Langchain is necessary. \n",
    "- Lack of data and lack of metrics are the key issues. \n",
    "- How LangChain is tackling this and will improve:\n",
    "  - For lack of data, there is a [LangChainDatassets](https://huggingface.co/LangChainDatasets) in Hugging Face. \n",
    "  - For lack of metrics, using no metrics, meaning just relying on the output and doing human observation. Next is using [tracing](https://js.langchain.com/v0.1/docs/modules/agents/how_to/logging_and_tracing/), a UI-based visualizer of your chain and agent runs. \n",
    "  - As I went through the SQL querying in the Tabular Data part, lets use the [SQL Question Answering Benchmarking: Chinook](https://python.langchain.com/v0.1/docs/use_cases/sql/large_db/)\n",
    "\n",
    "**Loading the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data with sql connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "try:\n",
    "    conn = mysql.connector.connect(\n",
    "        user='root',\n",
    "        password='Dhaval@87588',\n",
    "        host='localhost',\n",
    "        database='Chinook'  # Optional, specify if connecting to a specific database\n",
    "    )\n",
    "    print(\"Connected successfully!\")\n",
    "    conn.close()\n",
    "except mysql.connector.Error as e:\n",
    "    print(f\"Error connecting to MySQL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using MySQL\n",
    "mysql_uri = 'mysql+mysqlconnector://root:pass1234@localhost:3306/chinook'\n",
    "\n",
    "db = SQLDatabase.from_uri(mysql_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql database chain\n",
    "chain = SQLDatabaseChain(llm=llm, database=db, input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (0.23.3)\n",
      "Requirement already satisfied: packaging in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function datasets.arrow_dataset.Dataset.cleanup_cache_files(self) -> int>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DatasetGenerationError: An error occurred while generating the dataset\n",
    "\n",
    "# ~/.cache/huggingface/datasets delete\n",
    "\n",
    "from datasets import Dataset\n",
    "Dataset.cleanup_cache_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "from langchain.evaluation.loading  import load_dataset\n",
    "dataset = load_dataset(\"sql-qa-chinook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setting up a Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SQLDatabase\n",
    "from langchain_experimental.sql.base import SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql database chain\n",
    "chain = SQLDatabaseChain(llm=llm, database=db, input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# doing just one prediction to check\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chain(\u001b[43mdataset\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# doing just one prediction to check\n",
    "chain(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulk predictions\n",
    "predictions = []\n",
    "predicted_dataset = []\n",
    "error_dataset = []\n",
    "for data in dataset:\n",
    "    try:\n",
    "        predictions.append(chain(data))\n",
    "        predicted_dataset.append(data)\n",
    "    except:\n",
    "        error_dataset.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluate the performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "llm = OpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(predicted_dataset, predictions, question_key=\"question\", prediction_key=\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding graded output to preditions dict\n",
    "for i, prediction in enumerate(predictions):\n",
    "    prediction['grade'] = graded_outputs[i]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now getting a count of the grades\n",
    "from collections import Counter\n",
    "Counter([pred['grade'] for pred in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter datapoints to the incorrect examples\n",
    "incorrect = [pred for pred in predictions if pred['grade'] == \" INCORRECT\"]\n",
    "incorrect[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 🤔💻 Code Understanding\n",
    "\n",
    "- LLMs are good at code understanding. I hope you are already using it to creat code based on your query. For example, in ChatGPT and similar chatbots. You might have heard about [copilot](https://github.com/features/copilot)\n",
    "- LangChain is a useful tool designed to parse the GitHub code repositories.\n",
    "- Let's use [pandas-ai](https://github.com/gventuri/pandas-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pandas-ai'...\n",
      "remote: Enumerating objects: 13049, done.\u001b[K\n",
      "remote: Counting objects: 100% (2934/2934), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1340/1340), done.\u001b[K\n",
      "remote: Total 13049 (delta 1574), reused 2665 (delta 1458), pack-reused 10115\u001b[K\n",
      "Receiving objects: 100% (13049/13049), 37.63 MiB | 2.87 MiB/s, done.\n",
      "Resolving deltas: 100% (8790/8790), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/gventuri/pandas-ai.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = '/Users/dhavalantala/Desktop/langchain/langchain/pandas-ai'\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            pass\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dhavalantala/Desktop/langchain/langchain/pandas-ai'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1611 documents\n",
      "\n",
      "------ Start Document ------\n",
      "page_content='# ignore-words.txt\\nselectin' metadata={'source': '/Users/dhavalantala/Desktop/langchain/langchain/pandas-ai/ignore-words.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"You have {len(docs)} documents\\n\")\n",
    "print(\"------ Start Document ------\")\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Let's use Chroma for storing documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(docs[:100], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To instantiate the OpenAI LLM from the PandasAI library, you should import the `OpenAIAgent` class.\n"
     ]
    }
   ],
   "source": [
    "query = \"What class should I import from pandasai to instantiate OpenAI llm?\"\n",
    "output = qa.run(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 🤖 Agents\n",
    "- Agents can be used in variety of tasks and the use case are evolving with the advancement in LLMs.\n",
    "- You could even create your own agent based on Langchain documentation.\n",
    "- Check out my [Auto-GPT with LangChain video](https://youtu.be/imDfPmMKEjM).\n",
    "- Let's use `ArXiv API Tool`\n",
    "- What is arxiv --> [https://arxiv.org/](https://arxiv.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhavalantala/Desktop/langchain/langchainvenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "tools = load_tools(\n",
    "    [\"arxiv\"], \n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use arxiv to search for the paper with the identifier 1706.03762.\n",
      "Action: arxiv\n",
      "Action Input: 1706.03762\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks in an encoder-decoder configuration. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer, based\n",
      "solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to be\n",
      "superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014\n",
      "English-to-German translation task, improving over the existing best results,\n",
      "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French\n",
      "translation task, our model establishes a new single-model state-of-the-art\n",
      "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\n",
      "of the training costs of the best models from the literature. We show that the\n",
      "Transformer generalizes well to other tasks by applying it successfully to\n",
      "English constituency parsing both with large and limited training data.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The paper 1706.03762 is about the Transformer model, a network architecture based solely on attention mechanisms for sequence transduction tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The paper 1706.03762 is about the Transformer model, a network architecture based solely on attention mechanisms for sequence transduction tasks.'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://arxiv.org/abs/1706.03762\n",
    "agent_chain.run(\n",
    "    \"What's the paper 1706.03762 about?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2023-08-02\\nTitle: Attention Is All You Need\\nAuthors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\\nSummary: The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv = ArxivAPIWrapper()\n",
    "docs = arxiv.run(\"1706.03762\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2016-09-28\\nTitle: Unsupervised Neural Hidden Markov Models\\nAuthors: Ke Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu, Kevin Knight\\nSummary: In this work, we present the first results for neuralizing an Unsupervised\\nHidden Markov Model. We evaluate our approach on tag in- duction. Our approach\\noutperforms existing generative models and is competitive with the\\nstate-of-the-art though with a simpler model easily extended to include\\nadditional context.\\n\\nPublished: 2018-04-12\\nTitle: Self-Attention with Relative Position Representations\\nAuthors: Peter Shaw, Jakob Uszkoreit, Ashish Vaswani\\nSummary: Relying entirely on an attention mechanism, the Transformer introduced by\\nVaswani et al. (2017) achieves state-of-the-art results for machine\\ntranslation. In contrast to recurrent and convolutional neural networks, it\\ndoes not explicitly model relative or absolute position information in its\\nstructure. Instead, it requires adding representations of absolute positions to\\nits inputs. In this work we present an alternative approach, extending the\\nself-attention mechanism to efficiently consider representations of the\\nrelative positions, or distances between sequence elements. On the WMT 2014\\nEnglish-to-German and English-to-French translation tasks, this approach yields\\nimprovements of 1.3 BLEU and 0.3 BLEU over absolute position representations,\\nrespectively. Notably, we observe that combining relative and absolute position\\nrepresentations yields no further improvement in translation quality. We\\ndescribe an efficient implementation of our method and cast it as an instance\\nof relation-aware self-attention mechanisms that can generalize to arbitrary\\ngraph-labeled inputs.\\n\\nPublished: 2018-03-16\\nTitle: Tensor2Tensor for Neural Machine Translation\\nAuthors: Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, Jakob Uszkoreit\\nSummary: Tensor2Tensor is a library for deep learning models that is well-suited for\\nneural machine translation and includes the reference implementation of the\\nstate-of-the-art Transformer model.'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = arxiv.run(\"Ashish Vaswani\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No good Arxiv Result was found'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random input off-course will throw error.\n",
    "docs = arxiv.run(\"1605.08386WWW\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Happy Learning 😎**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
