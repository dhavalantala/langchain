{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Language Model Application: Chat Models\n",
    "\n",
    "- Chat models are a variation on language models. while chat models use language models under the hood, the interface they expose is a bit different: rather that expose a \"teext in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: LangChain provides many modelues that can be used to build language model application. Modules can be combined to create more compalex application, or be used individually for simple application. \n",
    "\n",
    "### Get Message Completions from a Chat Model\n",
    "\n",
    "- I can get chat complitions by passing one or more messages to chat model and response will be message. \n",
    "\n",
    "- The types of message currently supported in Langchain are `AIMessage`, `HumanMessage`, `SystemMessage`, and `ChatMessage` - `Chatmessage` takes in an arbitrary role parrameter. Most of the time, we'll just be dealling with `HumanMessage`, `AIMessage` and `SystemMessage`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='હું પ્રોગ્રામિંગ પસંદ કરું છું.', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 20, 'total_tokens': 73}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9321712f-71fe-4e92-afea-39ae5501322f-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now get completions by passing in a single message\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to Gujarati. I love programming.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'હું પ્રોગ્રામિંગ પસંદ કરું છું.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trns.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='હું પ્રોગ્રામિંગ પસંદ કરું છું.', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 36, 'total_tokens': 89}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ce1d2fee-0f82-4e39-a4d7-e1430aac0d14-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can pass multiple messages for OpenAI’s gpt-3.5-turbo and gpt-4 models\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to Gujarati.\"),\n",
    "    HumanMessage(content=\"Translate this sentence from English to Gujarati. I love programming.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='હું પ્રોગ્રામિંગ પસંદ કરું છું.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='હું પ્રોગ્રામિંગ પસંદ કરું છું.', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 36, 'total_tokens': 89}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b68fceb0-f8b3-4507-a9e2-f0cb3ce7b3fb-0'))], [ChatGeneration(text='હું કૃત્રિમ બુધિમત્તાને પ્રેમ કરું છું.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='હું કૃત્રિમ બુધિમત્તાને પ્રેમ કરું છું.', response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 37, 'total_tokens': 104}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-089cb22f-f5f7-4eea-907d-37c25f2b4c2f-0'))]], llm_output={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 73, 'total_tokens': 193}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('b68fceb0-f8b3-4507-a9e2-f0cb3ce7b3fb')), RunInfo(run_id=UUID('089cb22f-f5f7-4eea-907d-37c25f2b4c2f'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Gujarati.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to Gujarati. I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Gujarati.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to Gujarati. I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 120, 'prompt_tokens': 73, 'total_tokens': 193}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output['token_usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Templets\n",
    "\n",
    "- Insted of hard coding that text I want to ask, simillar to LLMs, I can use make of templeting by using a `MessageProptTemplate`\n",
    "\n",
    "- I can build a `ChatPromptTemplate` from one or more `MessagePromptTemplates`. \n",
    "\n",
    "- I can use `ChatPromptTemplate's` `format_prompt` - this return a `PromptValue`, Which Ican convert to a string or `Message` object, depending on whether you want to use the formatted values as input to an llm or chat model.\n",
    "\n",
    "- for simplicity, there is a `from_template` method exposed on the template which makes our task lot easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='હું પ્રોગ્રામિંગ પસંદ કરું છું.', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 26, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2e08a8c7-b8cf-4cd2-846a-bc808cb4cdb2-0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = '{text}'\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "#get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language= \"English\", output_language=\"Gujarati\", text=\"I love programming.\").to_messages())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains and Chat Models \n",
    "- In real applciation, using LLM in isolation is OK for some applicatopn but in most of the cases it requires chaining. And chianging with PtromptTemplate might be a neccessity. \n",
    "\n",
    "- A chain in LangChain is made up of links, which can either primitives like LLMs or other chains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'હું પ્રોગ્રામિંગ પસંદ કરું છું.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt = chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language = \"Gujarati\", text = \"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents Chat Models\n",
    "\n",
    "- So, far what we did was to run the chain in predetermined order. \n",
    "\n",
    "- Agent can also be used with chat models, you can initialize one using `AgentType`. `CHAT_ZERO_SHOT_REACT_DESCRIPTION` as the agent type.\n",
    "\n",
    "in order to load agents, understanding the following concepts is crucial. \n",
    "\n",
    "- **Tool**: A function that performs a specific duty. This can be thins like: Google Search, database llokpu, Python REMP, Other chains. \n",
    "\n",
    "- **ChatModels**: The chat models powering the agent. \n",
    "\n",
    "- **Agent**: Agent involve an LLm making decisions about which actions to take, taking that action, seeing an observation, and repeating that until its done. \n",
    "\n",
    "**Agent**: For a list supported agents and their specifications.\n",
    "\n",
    "**Tool**: For a list of predefined tools and their specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Who is the president of Finland? What is 2+2?\n",
      "Thought: The first question requires a search, while the second question involves a simple math calculation.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"President of Finland\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAlexander Stubb\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"2+2\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: Alexander Stubb, 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Alexander Stubb, 4'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm = llm)\n",
    "\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is the president of Finland ? What is 2+2 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Add State to Chains and Agents \n",
    "\n",
    "- So far, all the chains and agents we've gone through have been stateless. but often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions.\n",
    "\n",
    "- For example, while designing a chatbot you want it to remember previous message or previous several messages.\n",
    "\n",
    "- Short-term memory\n",
    "\n",
    "- Long-term-memory (remembering key pieces of information over time)\n",
    "\n",
    "- I can use Memory with chains and agents initialized with chat models. The main difference between this and Memory for LLMs is that rather than trying to condense all previous messages into a string, I can keep them as their own unique memory object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great to hear! I'm here to chat with you and answer any questions you might have. What's on your mind today?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an artificial intelligence designed to assist and provide information to users like you. I have been trained on a vast amount of data to help with a wide range of topics, from answering questions to providing recommendations. I'm constantly learning and improving to better serve your needs. Feel free to ask me anything you'd like to know!\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- The decision to use a chat model or an LLM would depend on the specific task you are trying to accomplish. Chat models are designed to have structured conversations with humans, so they would be useful for tasks like creating chatbots or customer service agents. On the other hand, LLMs are more general language models that can be used for a wide range of tasks, such as language translation, text generation, or summarization.\n",
    "\n",
    "- In general, if the task involves interacting with humans in a conversational way, a chat model would be the better choice. But if the task involves generating or processing large amounts of text, an LLM would be more appropriate. It's also worth noting that LLMs can be used in conjunction with other models, such as text embedding models, to improve their performance on specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
