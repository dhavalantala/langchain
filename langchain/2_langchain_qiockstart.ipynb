{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart Guide (Part_1)\n",
    "\n",
    "## Building a Language Model Application: LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs get prediction from a language model\n",
    "\n",
    "- This is the most basic building block of LangChain. \n",
    "- Provides a uniform interface to interact with different large language models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI, HuggingFaceHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The temperature argument in the OpenAI LLM wrapper is used to control the level of randomness in the generated text.\n",
    "\n",
    "- A higher temperature value will result in more diverse and unpredictable text, while a lower temperature value will result in more conservative and predictable text.\n",
    "\n",
    "- The default value for temperature is 1.0, and valid values range from 0.0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Spice Chill Co.\"\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9)  # model_name=\"text-davinci-003\"\n",
    "text = \"What would be a good company name for a company that makes coldring for summer in india with indian spices?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The default Hugging Face Hub inference APIs do not use specialized hardware. They are also not suitable for running larger models like (bigscience/bloom-560m or google/flan-t5-xxl)\n",
    "\n",
    "- In queries, the Hugging Face models (small, medium) do not check for grammar and spelling mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google/flan-t5-xl (too big for my system) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sasa\n"
     ]
    }
   ],
   "source": [
    "# initialize the wrapper around HuggingfaceHub models and call it on some input\n",
    "\n",
    "hub_llm = HuggingFaceHub(repo_id = \"google/flan-t5-small\", model_kwargs = {\"temperature\":0.9, \"max_length\":64}) # model_name=\"text-davinci-003\"\n",
    "text = \"What would be a good company name for a company that makes cold drink for summer in india with indian spices?\"\n",
    "print(hub_llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates: Mange propts for LLMs\n",
    "\n",
    "- Instead of hard-coding that text I want to aks, I can use prompt template to manage the prompt.\n",
    "- Also, when creating application, its not feasible to pass the input directly to LLM.\n",
    "\n",
    "- It's similar to `python f-strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm learning LangChain.\n"
     ]
    }
   ],
   "source": [
    "# python f-string example\n",
    "framework = \"LangChain\"\n",
    "print(f\"I'm learning {framework}.\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['product'],\n",
    "    template = \"What is a good name for company that makes {product} ?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for company that makes colorful shirts ?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product = \"colorful shirts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains: Compbine LLMs and Propts in multi-step workflows\n",
    "\n",
    "- In real application, Using LLM in isolation is OK for some applications but in most of the cases it requires chaining. And chaining with PromptTemplate might be a neccessity.\n",
    "\n",
    "- A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains\n",
    "\n",
    "- Extending the previous example, I can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9) # model_name=\"text-davinci-003\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Threads or Colorful Creations Co.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run(\"colorful shirts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents: Dynamically call chains based on user input\n",
    "\n",
    "- So, far what I did was to run the chains in a predetermined order. \n",
    "\n",
    "- Agents can use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning to the user. \n",
    "\n",
    "In order to load agents, understanding the following concepts is crucial. \n",
    "\n",
    "- LLM: The language model powering the agent. \n",
    "\n",
    "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
    "\n",
    "- Agents: Agents involve an LLM making decision about which actions to take, taking that action, seeing an observation, and repeating that until its done. \n",
    "\n",
    "Agents: for a list of supported agents and their specifications. \n",
    "\n",
    "Tools: For a list of predefined tools and their specifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for LangChain on a search engine to find out more information.\n",
      "Action: Search\n",
      "Action Input: LangChain\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Things to know'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should click on the first result to learn more.\n",
      "Action: Search\n",
      "Action Input: LangChain\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Things to know'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should read through the information provided to understand what LangChain is.\n",
      "Action: Search\n",
      "Action Input: LangChain\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Things to know'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: LangChain is a blockchain-based platform that aims to revolutionize the language learning industry by providing a decentralized and transparent ecosystem for language learners and teachers.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain is a blockchain-based platform that aims to revolutionize the language learning industry by providing a decentralized and transparent ecosystem for language learners and teachers.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "#tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "tools = load_tools([\"serpapi\"])\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What is LangChain ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
